{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderknyshov/Desktop/LLM/Data/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import locale\n",
    "import json\n",
    "import re\n",
    "import pymorphy2\n",
    "import torch\n",
    "import accelerate\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading JSON-dataset to dictionary\n",
    "with open('/Users/alexanderknyshov/Desktop/LLM/Data/datasets/train_set.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5177/5177 [00:02<00:00, 2003.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_text(text):\n",
    "    news_text = \"form a recommendation using news: \" + \" \".join(text['news'])\n",
    "    recommendation_text = \" \".join(text['recommendations'])\n",
    "    return {'input_text': news_text, 'target_text': recommendation_text}\n",
    "\n",
    "# Formatting original dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = dataset.map(format_text)\n",
    "# Removing unneccessery columns\n",
    "dataset = dataset.remove_columns(['news', 'recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4141/4141 [00:02<00:00, 1938.79 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1036/1036 [00:00<00:00, 2857.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and validate data\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data['news']) * split_ratio)\n",
    "\n",
    "train_news = data['news'][:split_index]\n",
    "train_recommendations = data['recommendations'][:split_index]\n",
    "train_data = {'news': train_news, 'recommendations': train_recommendations}\n",
    "\n",
    "val_news = data['news'][split_index:]\n",
    "val_recommendations = data['recommendations'][split_index:]\n",
    "val_data = {'news': val_news, 'recommendations': val_recommendations}\n",
    "\n",
    "# Returning to the format needed\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "train_dataset = train_dataset.map(format_text)\n",
    "train_dataset = train_dataset.remove_columns(['news', 'recommendations'])\n",
    "\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "val_dataset = val_dataset.map(format_text)\n",
    "val_dataset = val_dataset.remove_columns(['news', 'recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/alexanderknyshov/Desktop/LLM/Data/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4141/4141 [00:11<00:00, 352.34 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1036/1036 [00:01<00:00, 529.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('sberbank-ai/ruT5-base')\n",
    "\n",
    "# Method for data tokenization\n",
    "def tokenize_data(example):\n",
    "    input_encodings = tokenizer(example['input_text'], padding='max_length', truncation=True, max_length=512)\n",
    "    target_encodings = tokenizer(example['target_text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "    example['input_ids'] = input_encodings['input_ids']\n",
    "    example['attention_mask'] = input_encodings['attention_mask']\n",
    "    example['labels'] = target_encodings['input_ids']\n",
    "    return example\n",
    "\n",
    "# Train and validation data tokenization\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_data)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_data)\n",
    "\n",
    "# Getting rid of text columns\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                       # –†–∞–Ω–≥, –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–º–µ–Ω–µ–Ω –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏\n",
    "    lora_alpha=32,             # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å\n",
    "    lora_dropout=0.1,          # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞ –¥–ª—è LoRA\n",
    "    target_modules=[\"q\", \"v\"]  # –ú–æ–¥—É–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderknyshov/Desktop/LLM/Data/venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model Loading\n",
    "model = T5ForConditionalGeneration.from_pretrained('sberbank-ai/ruT5-base').to(device)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Defining training params\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= '/Users/alexanderknyshov/Desktop/LLM/Data/model',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=0.1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",   \n",
    "    logging_dir='/Users/alexanderknyshov/Desktop/LLM/Data/model/logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model='eval_loss',\n",
    ")\n",
    "\n",
    "# Metrics count\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    loss = sum([1 if pred == label else 0 for pred, label in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n",
    "    return {'accuracy': loss}\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 10/208 [00:05<01:48,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.4203, 'grad_norm': 21.546478271484375, 'learning_rate': 4.7596153846153844e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñâ         | 20/208 [00:11<01:42,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.8443, 'grad_norm': 4.979017734527588, 'learning_rate': 4.519230769230769e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 30/208 [00:16<01:36,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3059, 'grad_norm': 29.900989532470703, 'learning_rate': 4.278846153846154e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 40/208 [00:22<01:31,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6691, 'grad_norm': 3.5398900508880615, 'learning_rate': 4.038461538461539e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 50/208 [00:27<01:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1111, 'grad_norm': 4.124181270599365, 'learning_rate': 3.798076923076923e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñâ       | 60/208 [00:33<01:20,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.5617, 'grad_norm': 5.882956027984619, 'learning_rate': 3.557692307692308e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñé      | 70/208 [00:38<01:15,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3873, 'grad_norm': 5.712183952331543, 'learning_rate': 3.3173076923076926e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 80/208 [00:44<01:09,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2048, 'grad_norm': 3.3911314010620117, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 90/208 [00:49<01:04,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1016, 'grad_norm': 34.90683364868164, 'learning_rate': 2.8365384615384616e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 100/208 [00:54<00:58,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8675, 'grad_norm': 4.591811656951904, 'learning_rate': 2.5961538461538464e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 110/208 [01:00<00:53,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8825, 'grad_norm': 5.265198230743408, 'learning_rate': 2.355769230769231e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 120/208 [01:05<00:48,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8602, 'grad_norm': 9.170716285705566, 'learning_rate': 2.1153846153846154e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 130/208 [01:11<00:42,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.0077, 'grad_norm': 10.381199836730957, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 140/208 [01:16<00:37,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9006, 'grad_norm': 6.096067428588867, 'learning_rate': 1.6346153846153847e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 150/208 [01:22<00:31,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7937, 'grad_norm': 3.1444592475891113, 'learning_rate': 1.3942307692307693e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 160/208 [01:27<00:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1875, 'grad_norm': 4.248955726623535, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 170/208 [01:33<00:20,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8614, 'grad_norm': 5.978460311889648, 'learning_rate': 9.134615384615384e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 180/208 [01:38<00:15,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.4891, 'grad_norm': 3.844843864440918, 'learning_rate': 6.730769230769231e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 190/208 [01:44<00:09,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6538, 'grad_norm': 3.309844970703125, 'learning_rate': 4.326923076923077e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 200/208 [01:49<00:04,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.4033, 'grad_norm': 3.3401286602020264, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [01:55<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 115.4811, 'train_samples_per_second': 3.586, 'train_steps_per_second': 1.801, 'train_loss': 8.637595066657433, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "torch.mps.empty_cache()\n",
    "# Evaluation\n",
    "#eval_results = trainer.evaluate()\n",
    "\n",
    "# Results\n",
    "#print(f\"Validation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ó–∞–º–µ–Ω–∏—Ç–µ 'path/to/saved/model' –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "model = T5ForConditionalGeneration.from_pretrained('/Users/alexanderknyshov/Desktop/LLM/Data/model/checkpoint-208').to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('/Users/alexanderknyshov/Desktop/LLM/Data/model/checkpoint-208')\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"form a recommendation using news: \" + \" \".join([\"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è\", \"–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è\", \"–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å\", \"–ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ\", \"–≤—Ä–µ–º–µ–Ω–∏\", \"–ø—Ä–∏–≤–µ–ª–∞\", \"–ø–∞–¥–µ–Ω–∏—é\", \"–ø—Ä–æ–¥–∞–∂\", \"–æ–¥–µ–∂–¥—ã\", \"—Ä–æ—Å—Å–∏–∏\", \"—Å—Ä–∞–≤–Ω–µ–Ω–∏—é\", \"–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º\", \"–ø–µ—Ä–∏–æ–¥–æ–º\", \"–ø—Ä–æ—à–ª–æ–≥–æ\", \"–≥–æ–¥–∞\", \"–æ—Ç–º–µ—á–∞—é—Ç\", \"—ç–∫—Å–ø–µ—Ä—Ç—ã\", \"–º–∞–µ\", \"–≥–æ–¥–∞\", \"–æ–ø–µ—Ä–∏—Ä—É—é—Ç\", \"–¥–∞–Ω–Ω—ã–º–∏\", \"–ø–µ—Ä–∏–æ–¥\", \"—Ñ–µ–≤—Ä–∞–ª—è\", \"–∞–ø—Ä–µ–ª—è\", \"—Ç–∞–∫–æ–µ\", \"—Å–Ω–∏–∂–µ–Ω–∏–µ\", \"—Å–ø—Ä–æ—Å–∞\", \"—Å–≤—è–∑—ã–≤–∞—é—Ç\", \"–¥–≤—É–º—è\", \"—Ñ–∞–∫—Ç–æ—Ä–∞–º–∏\", \"–≤–æ–ø–µ—Ä–≤—ã—Ö\", \"—É—Ö–æ–¥–æ–º\", \"—Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ\", \"—Ä—ã–Ω–∫–∞\", \"–∫—Ä—É–ø–Ω—ã—Ö\", \"–∑–∞–ø–∞–¥–Ω—ã—Ö\", \"—Å–µ—Ç–µ–π\", \"–º–∞—Å—Å–º–∞—Ä–∫–µ—Ç–∞\", \"–≤–æ–≤—Ç–æ—Ä—ã—Ö\", \"–≤—ã–Ω—É–∂–¥–µ–Ω–Ω—ã–º\", \"—Å–Ω–∏–∂–µ–Ω–∏–µ–º\", \"—Ä–∞—Å—Ö–æ–¥–æ–≤\", \"–º–Ω–æ–≥–∏—Ö\", \"—Ä–æ—Å—Å–∏—è–Ω–≥–∞–∑–ø—Ä–æ–º\", \"—Å–º–∏—Ä–∏–ª—Å—è\", \"–Ω–æ–≤—ã–π\", \"—ç–∫—Å–ø–æ—Ä—Ç–Ω—ã–π\", \"–º–∞—Ä—à—Ä—É—Ç\", \"—Å–µ–≤–µ—Ä–Ω—ã–π\", \"–ø–æ—Ç–æ–∫\", \"–∫–æ—Ç–æ—Ä—ã–π\", \"–ø–æ—Å—Ç—Ä–æ–µ–Ω\", \"–∑–∞–ø—É—â–µ–Ω\", \"—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é\", \"—Å–º–æ–∂–µ—Ç\", \"–Ω–∞–±—Ä–∞—Ç—å\", \"–º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é\", \"—Å–∫–æ—Ä–æ—Å—Ç—å\", \"–ø–æ—Å—Ç–∞–≤–æ–∫\", \"–≥–æ–ª—É–±–æ–≥–æ\", \"—Ç–æ–ø–ª–∏–≤–∞\", \"–µ–≤—Ä–æ–ø—É\", \"–≥–æ–¥–∞\", \"—Å—Ä–æ–∫–∞\", \"—Ç—Ä—É–±–∞–º\", \"–ª—É—á—à–µ–º\", \"—Å–ª—É—á–∞–µ\", \"—Å–º–æ–∂–µ—Ç\", \"—ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è\", \"–ª–∏—à—å\", \"–ø–æ–ª–æ–≤–∏–Ω–∞\", \"–∑–∞—è–≤–ª–µ–Ω–Ω—ã—Ö\", \"–æ–±—ä–µ–º–æ–≤\", \"–ø—Ä–∏—á–µ–º\", \"–ª–∏—à—å\", \"—Å–ª—É—á–∞–µ\", \"—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ\", \"—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ\", \"–¥–∞—Å—Ç\", \"–≥–µ—Ä–º–∞–Ω–∏—è–µ–≤—Ä–æ—Å–æ—é–∑\", \"–ø–æ–ª–æ–Ω\", \"—Ä–µ—à–∏–º–æ—Å—Ç–∏\", \"–ø—Ä–∏–Ω—è—Ç—å\", \"—à–µ—Å—Ç–æ–π\", \"–ø–∞–∫–µ—Ç\", \"—Å–∞–Ω–∫—Ü–∏–π\", \"–∫–æ—Ç–æ—Ä–æ–º\", \"–∫–æ–Ω—Ü–∞\", \"–≥–æ–¥–∞\", \"–ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω\", \"–ø–æ–ª–Ω—ã–π\", \"–æ—Ç–∫–∞–∑\", \"–∑–∞–∫—É–ø–æ–∫\", \"—Ä–æ—Å—Å–∏–π—Å–∫–æ–π\", \"–Ω–µ—Ñ—Ç–∏\", \"–Ω–µ—Ñ—Ç–µ–ø—Ä–æ–¥—É–∫—Ç–æ–≤\", \"–º–∫\", \"–ø–æ–æ–±—â–∞–ª—Å—è\", \"—ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏\", \"–≤—ã—è—Å–Ω–∏—Ç—å\", \"—ç—Ç–æ\", \"–≤—ã–ª—å–µ—Ç—Å—è\", \"–±—é–¥–∂–µ—Ç–∞\", \"—Ä–æ—Å—Å–∏–∏\", \"–≤–∞–ª—é—Ç–Ω—ã—Ö\", \"–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–π–∑–∞–ø–∞–¥–Ω—ã–µ\", \"—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ\", \"–∏–∑–¥–∞–Ω–∏—è\", \"–ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç\", \"–≤–µ—Ä–∏—Ç—å\", \"—Ä–æ—Å—Å–∏—é\", \"–º–Ω–µ–Ω–∏—é\", \"–∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ\", \"–±—Ä–∏—Ç–∞–Ω—Å–∫–æ–≥–æ\", \"–∂—É—Ä–Ω–∞–ª–∞\", \"the\", \"economist\", \"–Ω–∞—à–∞\", \"—Å—Ç—Ä–∞–Ω–∞\", \"–Ω–µ—Å–º–æ—Ç—Ä—è\", \"—Å–≤—è–∑–∞–Ω–Ω—ã–µ\", \"—Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–º\", \"–ø—Ä–µ—Å—Å–∏–Ω–≥–æ–º\", \"–ø—Ä–æ–±–ª–µ–º—ã\", \"–Ω–∞—Ö–æ–¥–∏—Ç\", \"—Å–ø–æ—Å–æ–±—ã\", \"—Å–ø—Ä–∞–≤–∏—Ç—å—Å—è\", \"–ø—Ä–µ–¥—ä—è–≤–ª—è–µ–º—ã–º–∏\", \"–≤—ã–∑–æ–≤–∞–º–∏\", \"–Ω–∞–ø—Ä–∏–º–µ—Ä\", \"–ø–æ–¥–Ω—è—Ç–∞—è\", \"—Ä–µ–∫–æ—Ä–¥–Ω–æ–≥–æ\", \"—É—Ä–æ–≤–Ω—è\", \"–∫–ª—é—á–µ–≤–∞—è\", \"—Å—Ç–∞–≤–∫–∞\", \"—Ü–±\", \"–ø–æ–∑–≤–æ–ª–∏–ª–∞\", \"—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å\", \"–∫—É—Ä—Å\", \"—Ä—É–±–ª—è\", \"–≤–µ—Ä–Ω—É—Ç—å\", \"—Ä–æ—Å—Å–∏–π—Å–∫—É—é\", \"–≤–∞–ª—é—Ç—É\", \"—É—Ä–æ–≤–µ–Ω—å\", \"–æ–∫—Ç—è–±—Ä—è\", \"–ø—Ä–æ—à–ª–æ–≥–æ\", \"–≥–æ–¥–∞\", \"–µ–≤—Ä–æ–ø–∞\", \"–±–æ–∏—Ç—Å—è\", \"–æ—Å—Ç–∞—Ç—å—Å—è\", \"—Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö\", \"—Ä–µ—Å—É—Ä—Å–æ–≤\", \"–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞\", \"–µ—Å\", \"–Ω–∞—á–∞–ª–∞\", \"–º–∞—è\", \"—Ä–µ–∑–∫–æ\", \"–Ω–∞—Ä–∞—Å—Ç–∏–ª–∏\", \"–∑–∞–ø–∞—Å—ã\", \"–≥–∞–∑–∞\", \"—á–∞—Å—Ç–Ω–æ—Å—Ç–∏\", \"–≥–µ—Ä–º–∞–Ω–∏—è\", \"–Ω–∞—á–∞–ª–∞\", \"–∑–∞–ø–æ–ª–Ω—è—Ç—å\", \"–∫—Ä—É–ø–Ω–µ–π—à–µ–µ\", \"—Ö—Ä–∞–Ω–∏–ª–∏—â–µ\", \"–∑–∞–ø–∞–¥–Ω–æ–π\", \"–µ–≤—Ä–æ–ø–µ\", \"–ø–æ—Å—Ç–∞–≤–∫–∏\", \"—Ñ–∏–Ω–ª—è–Ω–¥–∏—é\", \"–≤—ã—Ä–æ—Å–ª–∏\", \"—Ä–∞–∑–∞\", \"–ø—Ä–∏—á–∏–Ω—ã\", \"–≥—Ä—è–¥—É—â–∏–π\", \"–æ—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π\", \"—Å–µ–∑–æ–Ω\", \"–∂–µ–ª–∞–Ω–∏–µ\", \"–µ–≤—Ä–æ–∫–æ–º–∏—Å—Å–∏–∏\", \"–≤–≤–µ—Å—Ç–∏\", \"—ç–º–±–∞—Ä–≥–æ\", \"–ø–æ—Å—Ç–∞–≤–∫–∏\", \"—Ä–æ—Å—Å–∏–∏\", \"–Ω–∞–∏–±–æ–ª–µ–µ\", \"—É—è–∑–≤–∏–º—ã–º–∏\", \"—Å–ª—É—á–∞–µ\", \"—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\", \"–∏–º–ø–æ—Ä—Ç–∞\", \"–æ–∫–∞–∂—É—Ç—Å—è\", \"–∞–≤—Å—Ç—Ä–∏—è\", \"—Ñ–∏–Ω–ª—è–Ω–¥–∏—è\", \"–ª–∏—Ç–≤–∞–ø–æ\", \"–¥–∞–Ω–Ω—ã–º\", \"–≥–∞–∑–µ—Ç—ã\", \"financial\", \"time\", \"–∑–∞–ø–∞–¥–Ω—ã–µ\", \"–±–∞–Ω–∫–∏\", \"–æ—Ç–ª–æ–∂–∏–ª–∏\", \"–º–ª—Ä–¥\", \"—É–±—ã—Ç–∫–∏\", \"—Å–ª—É—á–∞–µ\", \"—É—Ö–æ–¥–∞\", \"—Ä–æ—Å—Å–∏–∏\", \"–ø—Ä–∏—á–∏–Ω–æ–π\", \"—Å–≤–æ—Ä–∞—á–∏–≤–∞–Ω–∏—è\", \"–ø—Ä–∏–±—ã–ª—å–Ω–æ–≥–æ\", \"–±–∏–∑–Ω–µ—Å–∞\", \"–Ω–∞—à–µ–π\", \"—Å—Ç—Ä–∞–Ω–µ\", \"–º–∏—Ä–æ–≤—ã—Ö\", \"—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö\", \"–≥–∏–≥–∞–Ω—Ç–æ–≤\", \"—è–≤–ª—è—é—Ç—Å—è\", \"–≤–≤–µ–¥–µ–Ω–Ω—ã–µ\", \"–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ\", \"—Å–∞–Ω–∫—Ü–∏–∏\", \"–ø–æ—Å—Ç—Ä–∞–¥–∞–µ—Ç\", \"–±–∞–Ω–∫–æ–≤—Å–∫–∞—è\", \"—Å–∏—Å—Ç–µ–º–∞\", \"—Ä–æ—Å—Å–∏–∏\", \"–æ—Ç—Ä–∞–∑–∏—Ç—å—Å—è\", \"–∫–ª–∏–µ–Ω—Ç–∞—Ö\", \"–º–æ—â–Ω—ã–π\", \"—Ö–ª–æ–ø–æ–∫\", \"–¥–≤–µ—Ä—å—é\", \"—Å—Ç–æ—Ä–æ–Ω—ã\", \"–∫—Ä—É–ø–Ω–µ–π—à–∏—Ö\", \"–±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö\", \"–≥—Ä—É–ø–ø\", \"–º–∫\", \"–≤—ã—è—Å–Ω–∏–ª\", \"—ç–∫—Å–ø–µ—Ä—Ç–æ–≤\", \"–±–∞–Ω–∫\", \"—Ä–æ—Å—Å–∏–∏\", \"–Ω–∞—á–∞–ª\", \"–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å\", \"–Ω–∞–∏–±–æ–ª–µ–µ\", \"–ø–æ–ø—É–ª—è—Ä–Ω—ã–µ\", \"—Å—Ö–µ–º—ã\", \"–∫–æ—Ç–æ—Ä—ã–µ\", \"–∏—Å–ø–æ–ª—å–∑—É—é—Ç\", \"–º–æ—à–µ–Ω–Ω–∏–∫–∏\", \"–∫—Ä–∞–∂–∏\", \"–¥–µ–Ω–µ–≥\", \"—Å—á–µ—Ç–æ–≤\", \"–±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö\", \"–∫–∞—Ä—Ç\", \"—Ä–æ—Å—Å–∏—è–Ω\", \"–ø—Ä–µ—Å—Ç—É–ø–Ω–∏–∫–∏\", \"—Å—Ç—Ä–µ–º—è—Ç—Å—è\", \"–æ–∫–∞–∑–∞—Ç—å\", \"–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é\", \"–∂–µ—Ä—Ç–≤—É\", \"–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ\", \"–≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ\", \"–Ω–∞–¥–∞–≤–∏—Ç—å\", \"—ç–º–æ—Ü–∏–∏\", \"–æ—Ç–∫–ª—é—á–∏—Ç—å\", \"–∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ\", \"–º—ã—à–ª–µ–Ω–∏–µ\", \"–æ–±–æ—à–ª–∏\", \"–≤–Ω–∏–º–∞–Ω–∏–µ–º\", \"–ø–æ—Å–ª–µ–¥–Ω–∏–µ\", \"–Ω–æ–≤–æ—Å—Ç–∏\", \"—Å–∞–Ω–∫—Ü–∏—è—Ö\", \"–±–ª–æ–∫–∏—Ä–æ–≤–∫–µ\", \"–∫–∞—Ä—Ç\", \"—ç–∫—Å–ø–µ—Ä—Ç—ã\", \"—Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏\", \"–ø–æ–¥–¥–∞—Ç—å—Å—è\", \"–º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é\", \"–≥–æ–ª–æ–¥\", \"–≥–ª–∞–≤–Ω–∞—è\", \"—É–≥—Ä–æ–∑–∞\", \"–Ω—ã–Ω–µ—à–Ω–µ–π\", \"—Ä–æ—Å—Å–∏–π—Å–∫–æ–π\", \"—Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏\", \"–æ–¥–Ω–∞–∫–æ\", \"–ø–æ—Ö–æ–∂–µ\", \"–º—ã—Å–ª—å\", \"–≥–æ–¥—É\", \"–Ω–µ–ø–ª–æ—Ö–æ\", \"–ø–æ—Å–∞–¥–∏—Ç—å\", \"–∫–∞—Ä—Ç–æ—à–∫—É\", \"–ø–æ—Å–µ—Ç–∏–ª–∞\", \"–æ—á–µ–Ω—å\", \"–º–Ω–æ–≥–∏—Ö\", \"–≥–æ—Ä–æ–¥—Å–∫–∏—Ö\", \"–∂–∏—Ç–µ–ª–µ–π\", \"—Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ\", \"–ø–æ–¥–º–æ—Å–∫–æ–≤–Ω—ã—Ö\", \"—Ä—ã–Ω–∫–∞—Ö\", \"–æ–±—Ä–∞–∑–æ–≤–∞–ª—Å—è\", \"–¥–µ—Ñ–∏—Ü–∏—Ç\", \"—Å–µ–º–µ–Ω–Ω–æ–≥–æ\", \"–∫–∞—Ä—Ç–æ—Ñ–µ–ª—è\", \"—Ü–µ–Ω—ã\", \"–≤–∑–ª–µ—Ç–µ–ª–∏\", \"–ø—Ä–æ–¥–∞—é—Ç\", \"–∫–∞—Ä—Ç–æ—à–∫—É\", \"–ø–æ—Å–µ–≤\", \"—Ç—é–ª—å–ø–∞–Ω—ã\", \"–∫–ª—É–±–Ω—è–º–∏\", \"—Ä—É–±–ª–µ–π\", \"—à—Ç—É–∫—É\", \"–ø—Ä–∏–≤—ã–∫—à—É—é\", \"—Å–∞–Ω–∫—Ü–∏—è–º\", \"—Ä–æ—Å—Å–∏—é\", \"—Ç—Ä—É–¥–Ω–æ\", \"—É–¥–∏–≤–∏—Ç—å\", \"–∫–∞–∫–∏–º–∏—Ç–æ\", \"–Ω–æ–≤—ã–º–∏\", \"–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏\", \"–º–æ—Å–∫–≤–∞\", \"–Ω–∞—Ö–æ–¥–∏—Ç\", \"–∑–µ—Ä–∫–∞–ª—å–Ω—ã–π\", \"–∞—Å—Å–∏–º–µ—Ç—Ä–∏—á–Ω—ã–π\", \"–æ—Ç–≤–µ—Ç\", \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\", \"–Ω–µ—Å–µ—Ç\", \"–±–æ–ª—å—à–∏–µ\", \"—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ\", \"–ø–æ—Ç–µ—Ä–∏\", \"–æ–∂–∏–¥–∞–Ω–∏—è—Ö\", \"–æ—Ç–∫–∞–∑–∞\", \"–µ—Å\", \"–∑–∞–∫—É–ø–æ–∫\", \"—Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ\", \"–≥–∞–∑–∞\", \"–Ω–∞—à–ª–∞—Å—å\", \"—Å–≤–æ–∏\", \"–∫–æ–Ω—Ç—Ä–º–µ—Ä—ã\", \"–æ–±—ä–µ–º—ã\", \"–∫–æ—Ç–æ—Ä—ã–µ\", \"—Å–µ–≥–æ–¥–Ω—è\", \"–ø–æ—Å—Ç–∞–≤–ª—è–µ–º\", \"—Å—Ç—Ä–∞–Ω—ã\", \"–µ–≤—Ä–æ–ø—ã\", \"–º–æ–≥—É—Ç\", \"–ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã\", \"–∞–∑–∏–∞—Ç—Å–∫–æ—Ç–∏—Ö–æ–æ–∫–µ–∞–Ω—Å–∫–∏–π\", \"—Ä–µ–≥–∏–æ–Ω\", \"–∞—Ç—Ä\", \"—Å–±–µ—Ä–±–∞–Ω–∫\", \"–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ\", \"–æ—Ç—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª\", \"—Ä–µ—à–µ–Ω–∏–µ\", \"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞\", \"—Ä—Ñ\", \"—É–≤–µ–ª–∏—á–∏—Ç—å\", \"–º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é\", \"—Å—É–º–º—É\", \"–∫—Ä–µ–¥–∏—Ç–∞\", \"–∫–æ—Ç–æ—Ä—É—é\", \"–±—Ä–∞—Ç—å\", \"–ø–æ–∫—É–ø–∫—É\", \"–∫–≤–∞—Ä—Ç–∏—Ä—ã\", \"–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º\", \"–ª—å–≥–æ—Ç–Ω–æ–π\", \"—Å—Ç–∞–≤–∫–∏\", \"–º–∞—è\", \"–ø–æ–∫—É–ø–∞—Ç–µ–ª–∏\", \"–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏\", \"–º–æ—Å–∫–≤–µ\", \"–º–æ—Å–∫–æ–≤—Å–∫–æ–π\", \"–æ–±–ª–∞—Å—Ç–∏\", \"—Å–∞–Ω–∫—Ç–ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ\", \"–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–æ–π\", \"–æ–±–ª–∞—Å—Ç–∏\", \"–º–æ–≥—É—Ç\", \"—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å\", \"–∫—Ä–µ–¥–∏—Ç—ã\", \"–º–ª–Ω\", \"—Ä—É–±–ª–µ–π—Å—Ç—Ä–∞–Ω—ã\", \"–µ–≤—Ä–æ–ø—ã\", \"—Ä–∞—Å–∫–æ–ª–æ–ª–∏—Å—å\", \"–ª–∏–Ω–∏–∏\", \"—Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ\", \"–≥–∞–∑–∞\", \"–æ–¥–Ω–∏\", \"—Å–æ–≥–ª–∞—Å–Ω—ã\", \"—Å—Ö–µ–º—É\", \"–æ–ø–ª–∞—Ç—ã\", \"–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π\", \"—Ä—É–±–ª–∏\", \"–¥—Ä—É–≥–∏–µ\", \"–æ—Ç–∫–∞–∑–∞–ª–∏—Å—å\", \"–Ω–∞–ø—Ä–æ—á—å\", \"–≥–∞–∑\", \"—Ä–æ—Å—Å–∏–∏\", \"–ª–æ\", \"—Å–∏—Ö\", \"–ø–æ—Ä\", \"–∑–∞–∫—Ä—ã–≤–∞–ª\", \"—Ç—Ä–µ—Ç–∏\", \"–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π\", \"–∂–∏—Ç–µ–ª–µ–π\", \"–∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç–∞\", \"—ç–Ω–µ—Ä–≥–æ–Ω–æ—Å–∏—Ç–µ–ª—è—Ö\", \"–µ–≤—Ä–æ–ø–µ–π—Ü–∞–º\", \"–ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è\", \"–∏—Å–∫–∞—Ç—å\", \"–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö\", \"–ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤\", \"—Ç–æ–ø–ª–∏–≤–∞\", \"—Ä–æ—Å—Å–∏–∏\", \"–Ω—É–∂–Ω–æ\", \"–æ–±–µ—Å–ø–µ—á–∏—Ç—å\", \"–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π\", \"—Ä—ã–Ω–æ–∫\", \"—Å–±—ã—Ç–∞\", \"—Å–º–æ–∂–µ—Ç\", \"–Ω–∞—à–∞\", \"—Å—Ç—Ä–∞–Ω–∞\", \"–ø–æ–≤–µ—Ä–Ω—É—Ç—å\", \"—Å–≤–æ–π\", \"—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π\", \"–ø–æ—Ç–æ–∫\", \"–∑–∞–ø–∞–¥–∞\", \"–≤–æ—Å—Ç–æ–∫\", \"–æ–±—ä—è—Å–Ω–∏–ª–∏\", \"—ç–∫–æ–Ω–æ–º–∏—Å—Ç—ã–≤–µ—Ä—Ö–æ–≤–Ω—ã–π\", \"–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å\", \"–µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–≥–æ\", \"—Å–æ—é–∑–∞\", \"–∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–º\", \"–¥–µ–ª–∞–º\", \"–ø–æ–ª–∏—Ç–∏–∫–µ\", \"–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\", \"–∂–æ–∑–µ–ø\", \"–±–æ—Ä—Ä–µ–ª—å\", \"–ø—Ä–µ–¥–ª–æ–∂–∏–ª\", \"—Å—Ç—Ä–∞–Ω–∞–º—á–ª–µ–Ω–∞–º\", \"–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è\", \"–æ–±—Å—É–¥–∏—Ç—å\", \"–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å\", \"–∏–∑—ä—è—Ç–∏—è\", \"–∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö\", \"—Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö\", \"–≤–∞–ª—é—Ç–Ω—ã—Ö\", \"—Ä–µ–∑–µ—Ä–≤–æ–≤\", \"–ø–æ–¥–¥–µ—Ä–∂–∫–∏\", \"—É–∫—Ä–∞–∏–Ω—ã\", \"–º–∏–¥\", \"—Ä—Ñ\", \"—Ç–∞–∫—É—é\", \"–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—É\", \"–Ω–∞–∑–≤–∞–ª–∏\", \"–ø–æ–ª–Ω—ã–º\", \"–±–µ–∑–∑–∞–∫–æ–Ω–∏–µ–º\", \"—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ–º\", \"–æ—Å–Ω–æ–≤—ã\", \"–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö\", \"–æ—Ç–Ω–æ—à–µ–Ω–∏–π\", \"—ç–∫—Å–ø–µ—Ä—Ç\", \"–æ—Ü–µ–Ω–∏–ª\", \"–ø–æ–π–¥–µ—Ç\", \"–∑–∞–ø–∞–¥\", \"—á—Ä–µ–∑–≤—ã—á–∞–π–Ω—ã–π\", \"—à–∞–≥\", \"–æ—Ç–≤–µ—Ç–∏—Ç—å\", \"—Ä–æ—Å—Å–∏—è\", \"–±–æ–ª—å—à–∏–µ\", \"–º–∞–≥–∞–∑–∏–Ω—ã\", \"–∫–æ–º–ø–∞–Ω–∏–∏\", \"–∑–∞–∫—Ä—ã–≤–∞—é—Ç—Å—è\", \"—É—Ö–æ–¥—è—Ç\", \"—Ä–æ—Å—Å–∏–∏\", \"—Ñ–æ–Ω–µ\", \"—Å–∞–Ω–∫—Ü–∏–π\", \"–Ω–µ—Ä–µ–¥–∫–æ\", \"—Å–æ–∑–¥–∞–≤–∞—è\", \"–∫–æ–ª–æ—Å—Å–∞–ª—å–Ω—ã–π\", \"–æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π\", \"—Ä–µ–∑–æ–Ω–∞–Ω—Å\", \"—Ç–µ–Ω–∏\", \"–æ—Å—Ç–∞—ë—Ç—Å—è\", \"–º–∞–ª—ã–π\", \"–±–∏–∑–Ω–µ—Å\", \"–º–∫\", \"—Å–æ–±—Ä–∞–ª\", \"–∏—Å—Ç–æ—Ä–∏–∏\", \"–Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö\", \"–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π\", \"–∫–æ—Ç–æ—Ä—ã–µ\", \"—Å–µ–≥–æ–¥–Ω—è\", \"–æ—Å—Ç–∞–ª–∏—Å—å\", \"—Å–≤–æ–∏–º–∏\", \"–ø—Ä–æ–±–ª–µ–º–∞–º–∏\", \"–æ–ø—É—Å—Ç–∏–ª–∏\", \"—Ä—É–∫–∏\", \"—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è\", \"–≤–æ–µ–Ω–Ω–∞—è\", \"–æ–ø–µ—Ä–∞—Ü–∏—è\", \"—Ä–∞–∑–≤—è–∑–∞–Ω–Ω–∞—è\", \"–∑–∞–ø–∞–¥–æ–º\", \"—Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω–∞—è\", \"–ø–æ–ª–∏—Ç–∏–∫–∞\", \"–ø—Ä–æ—Ç–∏–≤\", \"—Ä–æ—Å—Å–∏–∏\", \"–æ—Ñ–æ—Ä–º–∏–ª–∏\", \"–∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ\", \"–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ\", \"—Ö—Ö\", \"–≤–µ–∫–∞\", \"–º–∏—Ä–æ–≤–æ–π\", \"–∞—Ä–µ–Ω–µ\", \"—Å–Ω–∏–∂–∞–µ—Ç—Å—è\", \"—á–∞—Å—Ç–æ\", \"—Å—Ö–æ–¥–∏—Ç\", \"—Ä–æ–ª—å\", \"—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö\", \"–º–∏—Ä–æ–≤—ã—Ö\", \"–ø–æ–ª–∏—Ç–∏–∫–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö\", \"–∏–≥—Ä–æ–∫–æ–≤–Ω—ã–Ω–µ—à–Ω–∏–π\", \"–≥–æ–¥\", \"—Å—Ç–∞—Ç—å\", \"—Å–∞–º—ã–º\", \"–ø—Ä–æ–≤–∞–ª—å–Ω—ã–º\", \"–æ—Ç—Ä–µ–∑–∫–æ–º\", \"–≤—Ä–µ–º–µ–Ω–∏\", \"—ç–∫–æ–Ω–æ–º–∏–∫–∏\", \"–Ω–∞—à–µ–π\", \"—Å—Ç—Ä–∞–Ω—ã\", \"–ø–æ—Å–ª–µ–¥–Ω–∏–µ\", \"–¥–µ—Å—è—Ç–∫–∞\", \"–ª–µ—Ç\", \"–¥–∞–Ω–Ω—ã–º\", \"–∞–≥–µ–Ω—Ç—Å—Ç–≤–∞\", \"bloomberg\", \"–Ω–µ–¥—Ä–∞—Ö\", \"–º–∏–Ω—Ñ–∏–Ω–∞\", \"—Ä—Ñ\", \"–ø—Ä–æ—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è\", \"–ø—Ä–æ–≥–Ω–æ–∑\", \"—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏\", \"–∫–æ—Ç–æ—Ä—ã–º\", \"–ø–æ—Ç–µ—Ä–∏\", \"–æ—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ\", \"–≤–≤–ø\", \"–º\", \"—Å–æ—Å—Ç–∞–≤—è—Ç\", \"–Ω–∞–±–ª—é–¥–∞–ª–æ—Å—å\", \"–≥–æ–¥–∞–º–∏–Ω—ç–∫–æ–Ω–æ–º—Ä–∞–∑–≤–∏—Ç–∏—è\", \"–ø–æ—Ä–∞–¥–æ–≤–∞–ª–æ\", \"—Ä–æ—Å—Å–∏—è–Ω\", \"–Ω–æ–≤—ã–º–∏\", \"—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏\", \"–≤—ã–∫–ª–∞–¥–∫–∞–º–∏\", \"–∏–Ω—Ñ–ª—è—Ü–∏—è\", \"—Å—Ç—Ä–∞–Ω–µ\", \"—Ä–∞–∑–æ–≥–Ω–∞–ª–∞—Å—å\", \"–≥–æ–¥–æ–≤–æ–º\", \"–≤—ã—Ä–∞–∂–µ–Ω–∏–∏\", \"—ç—Ç–æ\", \"–≤–¥–≤–æ–µ\", \"–ø—Ä–æ—à–ª–æ–≥–æ–¥–Ω–µ–≥–æ\", \"–∏–Ω–¥–µ–∫—Å–∞\", \"—Ä–æ—Å—Ç–∞\", \"–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö\", \"—Ü–µ–Ω\", \"–Ω–µ—Å–∫–æ–ª—å–∫–æ\", \"–≤—ã—à–µ\", \"—Ü–µ–ª–µ–≤—ã—Ö\", \"–ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π\", \"–¥–æ—Å—Ç–∏—á—å\", \"–∫–æ—Ç–æ—Ä—ã—Ö\", \"–æ–±–µ—â–∞–ª–æ\", \"–Ω–∞—Å–µ–ª–µ–Ω–∏—é\", \"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ—Ñ–æ–Ω–¥—ã\", \"–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ\", \"–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ\", \"–ø–µ–Ω—Å–∏–æ–Ω–Ω–æ–≥–æ\", \"—Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ\", \"—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è\", \"–ø–æ–ª—É—á–∞—Ç\", \"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞\", \"—Ç—Ä–ª–Ω\", \"—Ä—É–±–ª–µ–π\", \"–∫–∞—á–µ—Å—Ç–≤–µ\", \"–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏\", \"–æ—Ç—Å—Ä–æ—á–∫—É\", \"—É–ø–ª–∞—Ç—ã\", \"—Å—Ç—Ä–∞—Ö–æ–≤—ã—Ö\", \"–≤–∑–Ω–æ—Å–æ–≤\", \"–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é\", \"–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ–º\", \"—Ä—è–¥—É\", \"–æ—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö\", \"–ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π\", \"—Å–Ω–∏–∂–µ–Ω–∏—è\", \"—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π\", \"–Ω–∞–≥—Ä—É–∑–∫–∏\", \"—Å–ª–æ–≤–∞–º\", \"–ø—Ä–µ–º—å–µ—Ä–º–∏–Ω–∏—Å—Ç—Ä–∞\", \"–º–∏—Ö–∞–∏–ª–∞\", \"–º–∏—à—É—Å—Ç–∏–Ω–∞\", \"–¥–µ–Ω–µ–∂–Ω—ã–π\", \"—Ç—Ä–∞–Ω—à\", \"–ø–æ–∑–≤–æ–ª–∏—Ç\", \"—Å–æ—Ü–∏–∞–ª—å–Ω—ã–º\", \"—Ñ–æ–Ω–¥–∞–º\", \"–ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å\", \"–±–µ—Å–ø–µ—Ä–µ–±–æ–π–Ω—É—é\", \"–≤—ã–¥–∞—á—É\", \"–ª—é–¥—è–º\", \"–ø–µ–Ω—Å–∏–π\", \"–ø–æ—Å–æ–±–∏–π\", \"—Ç–∞–∫–∂–µ\", \"–ø–æ—Å–ª—É–∂–∏—Ç\", \"–∑–∞–ª–æ–≥–æ–º\", \"–æ–∫–∞–∑–∞–Ω–∏—è\", \"–±–µ—Å–ø–ª–∞—Ç–Ω–æ–π\", \"–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π\", \"–ø–æ–º–æ—â–∏–∏–¥–µ—è\", \"–æ–≤–ª–∞–¥–µ–≤—à–∞—è\", \"–º–∞—Å—Å–∞–º–∏\", \"—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è\", \"–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–π\", \"—Å–∏–ª–æ–π\", \"—É—á–∏–ª–∏\", \"–∫–ª–∞—Å—Å–∏–∫–∏\", \"–º–∞—Ä–∫—Å–∏–∑–º–∞\", \"—Ç–æ—á–∫–∏\", \"–∑—Ä–µ–Ω–∏—è\", \"–∏–¥–µ—è\", \"–∫–æ–Ω—Ñ–∏—Å–∫–∞—Ü–∏–∏\", \"—Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö\", \"–≤–∞–ª—é—Ç–Ω—ã—Ö\", \"—Ä–µ–∑–µ—Ä–≤–æ–≤\", \"–≤–µ—Å—å–º–∞\", \"–±–ª–∏–∑–∫–∞\", \"–≤–æ–ø–ª–æ—â–µ–Ω–∏—é\", \"—Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω–æ\", \"–ø–æ–∫–æ—Ä—è–µ—Ç\", \"–µ–≤—Ä–æ–∞—Ç–ª–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π\", \"–ø–æ–ª–∏—Ç–±–æ–º–æ–Ω–¥\", \"—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç\", \"—á–∞—Å—Ç–Ω–æ—Å—Ç–∏\", \"–Ω–µ–¥–∞–≤–Ω–∏–µ\", \"–≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è\", \"–≥–ª–∞–≤—ã\", \"–µ–≤—Ä–æ–¥–∏–ø–ª–æ–º–∞—Ç–∏–∏\", \"–∂–æ–∑–µ–ø–∞\", \"–±–æ—Ä—Ä–µ–ª—è\", \"—Ä–æ—Å—Å–∏—è\", \"—Å–ø–∞—Å—Ç–∏\", \"—Å–≤–æ–∏\", \"–∞—Ä–µ—Å—Ç–æ–≤–∞–Ω–Ω—ã–µ\", \"–∞–∫—Ç–∏–≤—ã\", \"—ç–∫—Å–ø—Ä–æ–ø—Ä–∏–∞—Ü–∏–∏\"])\n",
    "input_ids = tokenizer.encode(inp, return_tensors='pt').to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>–æ–≤ –Ω–∞ –æ—Ç–µ—á–µ—Å—Ç–≤–µ—Å—è —Ä—ã–Ω–∫ –Ω–µ–∑–∞–∫—É–ø–ª–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π –∏ —Ç–æ–≤–∞—Ä–µ –ø–æ-.gdd-–∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Ü–µ–Ω —Å –æ–º –≤ –∞—Ö –æ–≤. –≥—Ä–∏–≤–∞ sij–æ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():  # –í—ã–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "    outputs = model.generate(\n",
    "    input_ids,               # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    max_length=50,           # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    do_sample=True,          # –í–∫–ª—é—á–∞–µ–º —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è\n",
    "    temperature=1.2,         # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    top_k=50,                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±–æ—Ä –¥–æ 50 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Å–ª–æ–≤\n",
    "    top_p=0.9,               # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –æ—Ç—Å–µ—á–µ–Ω–∏–µ\n",
    "    no_repeat_ngram_size=2,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –±–∏–≥—Ä–∞–º–º\n",
    "    length_penalty=1.0       # –†–µ–≥—É–ª–∏—Ä—É–µ–º –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    ") # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ max_length –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "\n",
    "# –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤ —Ç–µ–∫—Å—Ç\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

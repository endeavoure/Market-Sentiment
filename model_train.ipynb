{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderknyshov/Desktop/LLM/Data/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import locale\n",
    "import json\n",
    "import re\n",
    "import pymorphy2\n",
    "import torch\n",
    "import accelerate\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading JSON-dataset to dictionary\n",
    "with open('/Users/alexanderknyshov/Desktop/LLM/Data/datasets/train_set.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5177/5177 [00:02<00:00, 2003.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_text(text):\n",
    "    news_text = \"form a recommendation using news: \" + \" \".join(text['news'])\n",
    "    recommendation_text = \" \".join(text['recommendations'])\n",
    "    return {'input_text': news_text, 'target_text': recommendation_text}\n",
    "\n",
    "# Formatting original dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = dataset.map(format_text)\n",
    "# Removing unneccessery columns\n",
    "dataset = dataset.remove_columns(['news', 'recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4141/4141 [00:02<00:00, 1938.79 examples/s]\n",
      "Map: 100%|██████████| 1036/1036 [00:00<00:00, 2857.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and validate data\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data['news']) * split_ratio)\n",
    "\n",
    "train_news = data['news'][:split_index]\n",
    "train_recommendations = data['recommendations'][:split_index]\n",
    "train_data = {'news': train_news, 'recommendations': train_recommendations}\n",
    "\n",
    "val_news = data['news'][split_index:]\n",
    "val_recommendations = data['recommendations'][split_index:]\n",
    "val_data = {'news': val_news, 'recommendations': val_recommendations}\n",
    "\n",
    "# Returning to the format needed\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "train_dataset = train_dataset.map(format_text)\n",
    "train_dataset = train_dataset.remove_columns(['news', 'recommendations'])\n",
    "\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "val_dataset = val_dataset.map(format_text)\n",
    "val_dataset = val_dataset.remove_columns(['news', 'recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/alexanderknyshov/Desktop/LLM/Data/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 4141/4141 [00:11<00:00, 352.34 examples/s]\n",
      "Map: 100%|██████████| 1036/1036 [00:01<00:00, 529.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('sberbank-ai/ruT5-base')\n",
    "\n",
    "# Method for data tokenization\n",
    "def tokenize_data(example):\n",
    "    input_encodings = tokenizer(example['input_text'], padding='max_length', truncation=True, max_length=512)\n",
    "    target_encodings = tokenizer(example['target_text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "    example['input_ids'] = input_encodings['input_ids']\n",
    "    example['attention_mask'] = input_encodings['attention_mask']\n",
    "    example['labels'] = target_encodings['input_ids']\n",
    "    return example\n",
    "\n",
    "# Train and validation data tokenization\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_data)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_data)\n",
    "\n",
    "# Getting rid of text columns\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                       # Ранг, может быть изменен в зависимости от задачи\n",
    "    lora_alpha=32,             # Гиперпараметр, который можно настроить\n",
    "    lora_dropout=0.1,          # Вероятность дропаута для LoRA\n",
    "    target_modules=[\"q\", \"v\"]  # Модули, которые будут адаптированы\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderknyshov/Desktop/LLM/Data/venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model Loading\n",
    "model = T5ForConditionalGeneration.from_pretrained('sberbank-ai/ruT5-base').to(device)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Defining training params\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= '/Users/alexanderknyshov/Desktop/LLM/Data/model',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=0.1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",   \n",
    "    logging_dir='/Users/alexanderknyshov/Desktop/LLM/Data/model/logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model='eval_loss',\n",
    ")\n",
    "\n",
    "# Metrics count\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    loss = sum([1 if pred == label else 0 for pred, label in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n",
    "    return {'accuracy': loss}\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 10/208 [00:05<01:48,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.4203, 'grad_norm': 21.546478271484375, 'learning_rate': 4.7596153846153844e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 20/208 [00:11<01:42,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.8443, 'grad_norm': 4.979017734527588, 'learning_rate': 4.519230769230769e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 30/208 [00:16<01:36,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3059, 'grad_norm': 29.900989532470703, 'learning_rate': 4.278846153846154e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 40/208 [00:22<01:31,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6691, 'grad_norm': 3.5398900508880615, 'learning_rate': 4.038461538461539e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 50/208 [00:27<01:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1111, 'grad_norm': 4.124181270599365, 'learning_rate': 3.798076923076923e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 60/208 [00:33<01:20,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.5617, 'grad_norm': 5.882956027984619, 'learning_rate': 3.557692307692308e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 70/208 [00:38<01:15,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3873, 'grad_norm': 5.712183952331543, 'learning_rate': 3.3173076923076926e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 80/208 [00:44<01:09,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2048, 'grad_norm': 3.3911314010620117, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 90/208 [00:49<01:04,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1016, 'grad_norm': 34.90683364868164, 'learning_rate': 2.8365384615384616e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 100/208 [00:54<00:58,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8675, 'grad_norm': 4.591811656951904, 'learning_rate': 2.5961538461538464e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 110/208 [01:00<00:53,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8825, 'grad_norm': 5.265198230743408, 'learning_rate': 2.355769230769231e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 120/208 [01:05<00:48,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8602, 'grad_norm': 9.170716285705566, 'learning_rate': 2.1153846153846154e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 130/208 [01:11<00:42,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.0077, 'grad_norm': 10.381199836730957, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 140/208 [01:16<00:37,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9006, 'grad_norm': 6.096067428588867, 'learning_rate': 1.6346153846153847e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 150/208 [01:22<00:31,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7937, 'grad_norm': 3.1444592475891113, 'learning_rate': 1.3942307692307693e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 160/208 [01:27<00:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1875, 'grad_norm': 4.248955726623535, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 170/208 [01:33<00:20,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8614, 'grad_norm': 5.978460311889648, 'learning_rate': 9.134615384615384e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 180/208 [01:38<00:15,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.4891, 'grad_norm': 3.844843864440918, 'learning_rate': 6.730769230769231e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 190/208 [01:44<00:09,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6538, 'grad_norm': 3.309844970703125, 'learning_rate': 4.326923076923077e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 200/208 [01:49<00:04,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.4033, 'grad_norm': 3.3401286602020264, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [01:55<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 115.4811, 'train_samples_per_second': 3.586, 'train_steps_per_second': 1.801, 'train_loss': 8.637595066657433, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "torch.mps.empty_cache()\n",
    "# Evaluation\n",
    "#eval_results = trainer.evaluate()\n",
    "\n",
    "# Results\n",
    "#print(f\"Validation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Замените 'path/to/saved/model' на путь к вашей сгенерированной модели\n",
    "model = T5ForConditionalGeneration.from_pretrained('/Users/alexanderknyshov/Desktop/LLM/Data/model/checkpoint-208').to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('/Users/alexanderknyshov/Desktop/LLM/Data/model/checkpoint-208')\n",
    "\n",
    "# Переводим модель в режим оценки\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"form a recommendation using news: \" + \" \".join([\"экономическая\", \"политическая\", \"нестабильность\", \"последнего\", \"времени\", \"привела\", \"падению\", \"продаж\", \"одежды\", \"россии\", \"сравнению\", \"аналогичным\", \"периодом\", \"прошлого\", \"года\", \"отмечают\", \"эксперты\", \"мае\", \"года\", \"оперируют\", \"данными\", \"период\", \"февраля\", \"апреля\", \"такое\", \"снижение\", \"спроса\", \"связывают\", \"двумя\", \"факторами\", \"вопервых\", \"уходом\", \"российского\", \"рынка\", \"крупных\", \"западных\", \"сетей\", \"массмаркета\", \"вовторых\", \"вынужденным\", \"снижением\", \"расходов\", \"многих\", \"россиянгазпром\", \"смирился\", \"новый\", \"экспортный\", \"маршрут\", \"северный\", \"поток\", \"который\", \"построен\", \"запущен\", \"эксплуатацию\", \"сможет\", \"набрать\", \"максимальную\", \"скорость\", \"поставок\", \"голубого\", \"топлива\", \"европу\", \"года\", \"срока\", \"трубам\", \"лучшем\", \"случае\", \"сможет\", \"экспортироваться\", \"лишь\", \"половина\", \"заявленных\", \"объемов\", \"причем\", \"лишь\", \"случае\", \"соответствующее\", \"разрешение\", \"даст\", \"германияевросоюз\", \"полон\", \"решимости\", \"принять\", \"шестой\", \"пакет\", \"санкций\", \"котором\", \"конца\", \"года\", \"предусмотрен\", \"полный\", \"отказ\", \"закупок\", \"российской\", \"нефти\", \"нефтепродуктов\", \"мк\", \"пообщался\", \"экспертами\", \"выяснить\", \"это\", \"выльется\", \"бюджета\", \"россии\", \"валютных\", \"поступленийзападные\", \"экономические\", \"издания\", \"продолжают\", \"верить\", \"россию\", \"мнению\", \"авторитетного\", \"британского\", \"журнала\", \"the\", \"economist\", \"наша\", \"страна\", \"несмотря\", \"связанные\", \"санкционным\", \"прессингом\", \"проблемы\", \"находит\", \"способы\", \"справиться\", \"предъявляемыми\", \"вызовами\", \"например\", \"поднятая\", \"рекордного\", \"уровня\", \"ключевая\", \"ставка\", \"цб\", \"позволила\", \"стабилизировать\", \"курс\", \"рубля\", \"вернуть\", \"российскую\", \"валюту\", \"уровень\", \"октября\", \"прошлого\", \"года\", \"европа\", \"боится\", \"остаться\", \"российских\", \"ресурсов\", \"государства\", \"ес\", \"начала\", \"мая\", \"резко\", \"нарастили\", \"запасы\", \"газа\", \"частности\", \"германия\", \"начала\", \"заполнять\", \"крупнейшее\", \"хранилище\", \"западной\", \"европе\", \"поставки\", \"финляндию\", \"выросли\", \"раза\", \"причины\", \"грядущий\", \"отопительный\", \"сезон\", \"желание\", \"еврокомиссии\", \"ввести\", \"эмбарго\", \"поставки\", \"россии\", \"наиболее\", \"уязвимыми\", \"случае\", \"сокращения\", \"импорта\", \"окажутся\", \"австрия\", \"финляндия\", \"литвапо\", \"данным\", \"газеты\", \"financial\", \"time\", \"западные\", \"банки\", \"отложили\", \"млрд\", \"убытки\", \"случае\", \"ухода\", \"россии\", \"причиной\", \"сворачивания\", \"прибыльного\", \"бизнеса\", \"нашей\", \"стране\", \"мировых\", \"финансовых\", \"гигантов\", \"являются\", \"введенные\", \"международные\", \"санкции\", \"пострадает\", \"банковская\", \"система\", \"россии\", \"отразиться\", \"клиентах\", \"мощный\", \"хлопок\", \"дверью\", \"стороны\", \"крупнейших\", \"банковских\", \"групп\", \"мк\", \"выяснил\", \"экспертов\", \"банк\", \"россии\", \"начал\", \"публиковать\", \"наиболее\", \"популярные\", \"схемы\", \"которые\", \"используют\", \"мошенники\", \"кражи\", \"денег\", \"счетов\", \"банковских\", \"карт\", \"россиян\", \"преступники\", \"стремятся\", \"оказать\", \"потенциальную\", \"жертву\", \"психологическое\", \"воздействие\", \"надавить\", \"эмоции\", \"отключить\", \"критическое\", \"мышление\", \"обошли\", \"вниманием\", \"последние\", \"новости\", \"санкциях\", \"блокировке\", \"карт\", \"эксперты\", \"рассказали\", \"поддаться\", \"манипулированию\", \"голод\", \"главная\", \"угроза\", \"нынешней\", \"российской\", \"турбулентности\", \"однако\", \"похоже\", \"мысль\", \"году\", \"неплохо\", \"посадить\", \"картошку\", \"посетила\", \"очень\", \"многих\", \"городских\", \"жителей\", \"результате\", \"подмосковных\", \"рынках\", \"образовался\", \"дефицит\", \"семенного\", \"картофеля\", \"цены\", \"взлетели\", \"продают\", \"картошку\", \"посев\", \"тюльпаны\", \"клубнями\", \"рублей\", \"штуку\", \"привыкшую\", \"санкциям\", \"россию\", \"трудно\", \"удивить\", \"какимито\", \"новыми\", \"ограничениями\", \"москва\", \"находит\", \"зеркальный\", \"ассиметричный\", \"ответ\", \"неизвестно\", \"несет\", \"большие\", \"финансовые\", \"потери\", \"ожиданиях\", \"отказа\", \"ес\", \"закупок\", \"российского\", \"газа\", \"нашлась\", \"свои\", \"контрмеры\", \"объемы\", \"которые\", \"сегодня\", \"поставляем\", \"страны\", \"европы\", \"могут\", \"перенаправлены\", \"азиатскотихоокеанский\", \"регион\", \"атр\", \"сбербанк\", \"оперативно\", \"отреагировал\", \"решение\", \"правительства\", \"рф\", \"увеличить\", \"максимальную\", \"сумму\", \"кредита\", \"которую\", \"брать\", \"покупку\", \"квартиры\", \"использованием\", \"льготной\", \"ставки\", \"мая\", \"покупатели\", \"недвижимости\", \"москве\", \"московской\", \"области\", \"санктпетербурге\", \"ленинградской\", \"области\", \"могут\", \"рассчитывать\", \"кредиты\", \"млн\", \"рублейстраны\", \"европы\", \"раскололись\", \"линии\", \"российского\", \"газа\", \"одни\", \"согласны\", \"схему\", \"оплаты\", \"конвертацией\", \"рубли\", \"другие\", \"отказались\", \"напрочь\", \"газ\", \"россии\", \"ло\", \"сих\", \"пор\", \"закрывал\", \"трети\", \"потребностей\", \"жителей\", \"континента\", \"энергоносителях\", \"европейцам\", \"приходится\", \"искать\", \"альтернативных\", \"поставщиков\", \"топлива\", \"россии\", \"нужно\", \"обеспечить\", \"аналогичный\", \"рынок\", \"сбыта\", \"сможет\", \"наша\", \"страна\", \"повернуть\", \"свой\", \"энергетический\", \"поток\", \"запада\", \"восток\", \"объяснили\", \"экономистыверховный\", \"представитель\", \"европейского\", \"союза\", \"иностранным\", \"делам\", \"политике\", \"безопасности\", \"жозеп\", \"боррель\", \"предложил\", \"странамчленам\", \"объединения\", \"обсудить\", \"возможность\", \"изъятия\", \"замороженных\", \"российских\", \"валютных\", \"резервов\", \"поддержки\", \"украины\", \"мид\", \"рф\", \"такую\", \"инициативу\", \"назвали\", \"полным\", \"беззаконием\", \"разрушением\", \"основы\", \"международных\", \"отношений\", \"эксперт\", \"оценил\", \"пойдет\", \"запад\", \"чрезвычайный\", \"шаг\", \"ответить\", \"россия\", \"большие\", \"магазины\", \"компании\", \"закрываются\", \"уходят\", \"россии\", \"фоне\", \"санкций\", \"нередко\", \"создавая\", \"колоссальный\", \"общественный\", \"резонанс\", \"тени\", \"остаётся\", \"малый\", \"бизнес\", \"мк\", \"собрал\", \"истории\", \"нескольких\", \"предпринимателей\", \"которые\", \"сегодня\", \"остались\", \"своими\", \"проблемами\", \"опустили\", \"руки\", \"специальная\", \"военная\", \"операция\", \"развязанная\", \"западом\", \"санкционная\", \"политика\", \"против\", \"россии\", \"оформили\", \"завершение\", \"политического\", \"хх\", \"века\", \"мировой\", \"арене\", \"снижается\", \"часто\", \"сходит\", \"роль\", \"традиционных\", \"мировых\", \"политикоэкономических\", \"игроковнынешний\", \"год\", \"стать\", \"самым\", \"провальным\", \"отрезком\", \"времени\", \"экономики\", \"нашей\", \"страны\", \"последние\", \"десятка\", \"лет\", \"данным\", \"агентства\", \"bloomberg\", \"недрах\", \"минфина\", \"рф\", \"прорабатывается\", \"прогноз\", \"соответствии\", \"которым\", \"потери\", \"отечественного\", \"ввп\", \"м\", \"составят\", \"наблюдалось\", \"годаминэкономразвития\", \"порадовало\", \"россиян\", \"новыми\", \"статистическими\", \"выкладками\", \"инфляция\", \"стране\", \"разогналась\", \"годовом\", \"выражении\", \"это\", \"вдвое\", \"прошлогоднего\", \"индекса\", \"роста\", \"потребительских\", \"цен\", \"несколько\", \"выше\", \"целевых\", \"показателей\", \"достичь\", \"которых\", \"обещало\", \"населению\", \"правительствофонды\", \"обязательного\", \"медицинского\", \"пенсионного\", \"социального\", \"страхования\", \"получат\", \"правительства\", \"трлн\", \"рублей\", \"качестве\", \"компенсации\", \"отсрочку\", \"уплаты\", \"страховых\", \"взносов\", \"предоставленную\", \"государством\", \"ряду\", \"отечественных\", \"предприятий\", \"снижения\", \"финансовой\", \"нагрузки\", \"словам\", \"премьерминистра\", \"михаила\", \"мишустина\", \"денежный\", \"транш\", \"позволит\", \"социальным\", \"фондам\", \"продолжить\", \"бесперебойную\", \"выдачу\", \"людям\", \"пенсий\", \"пособий\", \"также\", \"послужит\", \"залогом\", \"оказания\", \"бесплатной\", \"медицинской\", \"помощиидея\", \"овладевшая\", \"массами\", \"становится\", \"материальной\", \"силой\", \"учили\", \"классики\", \"марксизма\", \"точки\", \"зрения\", \"идея\", \"конфискации\", \"российских\", \"валютных\", \"резервов\", \"весьма\", \"близка\", \"воплощению\", \"стремительно\", \"покоряет\", \"евроатлантический\", \"политбомонд\", \"свидетельствуют\", \"частности\", \"недавние\", \"высказывания\", \"главы\", \"евродипломатии\", \"жозепа\", \"борреля\", \"россия\", \"спасти\", \"свои\", \"арестованные\", \"активы\", \"экспроприации\"])\n",
    "input_ids = tokenizer.encode(inp, return_tensors='pt').to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>ов на отечествеся рынк незакупленных товарных позиций и товаре по-.gdd-контрактопродуктов цен с ом в ах ов. грива sijо'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():  # Выключаем градиенты для оценки\n",
    "    outputs = model.generate(\n",
    "    input_ids,               # Входные данные\n",
    "    max_length=50,           # Максимальная длина генерации\n",
    "    do_sample=True,          # Включаем семплирование для большего разнообразия\n",
    "    temperature=1.2,         # Увеличиваем случайность при генерации\n",
    "    top_k=50,                # Ограничиваем выбор до 50 наиболее вероятных слов\n",
    "    top_p=0.9,               # Вероятностное отсечение\n",
    "    no_repeat_ngram_size=2,  # Предотвращаем повторение одинаковых биграмм\n",
    "    length_penalty=1.0       # Регулируем длину генерации\n",
    ") # Установите max_length по мере необходимости\n",
    "\n",
    "# Декодируем сгенерированные идентификаторы в текст\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
